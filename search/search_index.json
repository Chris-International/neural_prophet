{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Quick Start Guide This page contains details of how you can build a simple model using NeuralProphet with minimal features. Install After downloading the code repository (via git clone ), change to the repository directory ( cd neural_prophet ) and install neuralprophet as python package with pip install . Note: If you plan to use the package in a Jupyter notebook, it is recommended to install the 'live' package version with pip install .[live] . This will allow you to enable plot_live_loss in the train function to get a live plot of train (and validation) loss. Import Now you can use NeuralProphet in your code: from neuralprophet import NeuralProphet Input Data The input data format expected by the neural_prophet package is the same as in original prophet . It should have two columns, ds which has the timestamps and y column which contains the observed values of the time series. Throughout this documentation, we will be using the time series data of the log daily page views for the Peyton Manning Wikipedia page. The data can be imported as follows. import pandas as pd df = pd.read_csv('../example_data/example_wp_log_peyton_manning.csv') The format of the data looks like below. ds y 2007-12-10 9.59 2007-12-11 8.52 2007-12-12 8.18 2007-12-13 8.07 2007-12-14 7.89 Simple Model A simple model with neural_prophet for this dataset can be fitted by creating an object of the NeuralProphet class as follows and calling the fit function. This fits a model with the default settings in the model. For more details on these default settings, refer to the Section on Hyperparameter Selction . m = NeuralProphet() metrics = m.fit(df, freq=\"D\") Once the model is fitted, we can make forecasts using the fitted model. For this, we first need to create a future dataframe consisting of the time steps into the future that we need to forecast for. NeuralProphet provides the helper function make_future_dataframe for this purpose. Note that the the frequency of data is set globally here. Valid timeseries frequency settings are pandas timeseries offset aliases . future = m.make_future_dataframe(df, periods=365) forecast = m.predict(future) Plotting With the forecasts obtained from the model, you can visualize them. forecasts_plot = m.plot(forecast) This is a simple model with a trend, a weekly seasonality and a yearly seasonality estimated by default. You can also look at the individual components separately as below. fig_comp = m.plot_components(forecast) The individual coefficient values can also be plotted as below to gain further insights. fig_param = m.plot_parameters() Validation Model validation for NeuralProphet can be done in two ways. Users can split the dataset manually to validate after the model fitting like below by specifying the fraction of data to be used for validation in the argument valida_p . This validation set is reserved from the end of the series. m = NeuralProphet() df_train, df_val = m.split_df(df, valid_p=0.2) You can now look at the training and validation metrics separately as below. train_metrics = m.fit(df_train) val_metrics = m.test(df_val) You can also perform validation per every epoch during model fitting. This is done as follows by setting the validate_each_epoch argument in the fit function call. This lets you look at the validation metrics while model training. # or evaluate while training m = NeuralProphet() metrics = m.fit(df, validate_each_epoch=True, valid_p=0.2) Reproducibility The variability of results comes from SGD finding different optima on different runs. The majority of the randomness comes from the random initialization of weights, different learning rates and different shuffling of the dataloader. We can control the random number generator by setting it's seed: from neuralprophet import set_random_seed set_random_seed(0) This should lead to identical results every time you run the model. Note that you have to explicitly set the random seed to the same random number each time before fitting the model.","title":"Quickstart"},{"location":"#quick-start-guide","text":"This page contains details of how you can build a simple model using NeuralProphet with minimal features.","title":"Quick Start Guide"},{"location":"#install","text":"After downloading the code repository (via git clone ), change to the repository directory ( cd neural_prophet ) and install neuralprophet as python package with pip install . Note: If you plan to use the package in a Jupyter notebook, it is recommended to install the 'live' package version with pip install .[live] . This will allow you to enable plot_live_loss in the train function to get a live plot of train (and validation) loss.","title":"Install"},{"location":"#import","text":"Now you can use NeuralProphet in your code: from neuralprophet import NeuralProphet","title":"Import"},{"location":"#input-data","text":"The input data format expected by the neural_prophet package is the same as in original prophet . It should have two columns, ds which has the timestamps and y column which contains the observed values of the time series. Throughout this documentation, we will be using the time series data of the log daily page views for the Peyton Manning Wikipedia page. The data can be imported as follows. import pandas as pd df = pd.read_csv('../example_data/example_wp_log_peyton_manning.csv') The format of the data looks like below. ds y 2007-12-10 9.59 2007-12-11 8.52 2007-12-12 8.18 2007-12-13 8.07 2007-12-14 7.89","title":"Input Data"},{"location":"#simple-model","text":"A simple model with neural_prophet for this dataset can be fitted by creating an object of the NeuralProphet class as follows and calling the fit function. This fits a model with the default settings in the model. For more details on these default settings, refer to the Section on Hyperparameter Selction . m = NeuralProphet() metrics = m.fit(df, freq=\"D\") Once the model is fitted, we can make forecasts using the fitted model. For this, we first need to create a future dataframe consisting of the time steps into the future that we need to forecast for. NeuralProphet provides the helper function make_future_dataframe for this purpose. Note that the the frequency of data is set globally here. Valid timeseries frequency settings are pandas timeseries offset aliases . future = m.make_future_dataframe(df, periods=365) forecast = m.predict(future)","title":"Simple Model"},{"location":"#plotting","text":"With the forecasts obtained from the model, you can visualize them. forecasts_plot = m.plot(forecast) This is a simple model with a trend, a weekly seasonality and a yearly seasonality estimated by default. You can also look at the individual components separately as below. fig_comp = m.plot_components(forecast) The individual coefficient values can also be plotted as below to gain further insights. fig_param = m.plot_parameters()","title":"Plotting"},{"location":"#validation","text":"Model validation for NeuralProphet can be done in two ways. Users can split the dataset manually to validate after the model fitting like below by specifying the fraction of data to be used for validation in the argument valida_p . This validation set is reserved from the end of the series. m = NeuralProphet() df_train, df_val = m.split_df(df, valid_p=0.2) You can now look at the training and validation metrics separately as below. train_metrics = m.fit(df_train) val_metrics = m.test(df_val) You can also perform validation per every epoch during model fitting. This is done as follows by setting the validate_each_epoch argument in the fit function call. This lets you look at the validation metrics while model training. # or evaluate while training m = NeuralProphet() metrics = m.fit(df, validate_each_epoch=True, valid_p=0.2)","title":"Validation"},{"location":"#reproducibility","text":"The variability of results comes from SGD finding different optima on different runs. The majority of the randomness comes from the random initialization of weights, different learning rates and different shuffling of the dataloader. We can control the random number generator by setting it's seed: from neuralprophet import set_random_seed set_random_seed(0) This should lead to identical results every time you run the model. Note that you have to explicitly set the random seed to the same random number each time before fitting the model.","title":"Reproducibility"},{"location":"changes-from-prophet/","text":"What has Changed from Prophet NeuralProphet has a number of added features with respect to original Prophet. They are as follows. Gradient Descent for optimisation via using PyTorch as the backend. Modelling autocorrelation of time series using AR-Net Modelling lagged regressors using a sepearate Feed-Forward Neural Network. Configurable non-linear deep layers of the FFNNs. Tuneable to specific forecast horizons (greater than 1). Custom losses and metrics. Due to the modularity of the code and the extensibility supported by PyTorch, any component trainable by gradient descent can be added as a module to NeuralProphet. Using PyTorch as the backend, makes the modelling process much faster compared to original Prophet which uses Stan as the backend.","title":"Changes from Prophet"},{"location":"changes-from-prophet/#what-has-changed-from-prophet","text":"NeuralProphet has a number of added features with respect to original Prophet. They are as follows. Gradient Descent for optimisation via using PyTorch as the backend. Modelling autocorrelation of time series using AR-Net Modelling lagged regressors using a sepearate Feed-Forward Neural Network. Configurable non-linear deep layers of the FFNNs. Tuneable to specific forecast horizons (greater than 1). Custom losses and metrics. Due to the modularity of the code and the extensibility supported by PyTorch, any component trainable by gradient descent can be added as a module to NeuralProphet. Using PyTorch as the backend, makes the modelling process much faster compared to original Prophet which uses Stan as the backend.","title":"What has Changed from Prophet"},{"location":"contribute/","text":"Contribute Dev Install After downloading the code repository (via git clone ), change to the repository directory ( cd neural_prophet ), activate your virtual environment, and install neuralprophet as python package with pip install -e .[dev] (Including the optional -e flag will install neuralprophet in \"editable\" mode, meaning that instead of copying the files into your virtual environment, a symlink will be created to the files where they are.) Additionally you must run $ neuralprophet_dev_setup in your console to run the dev-setup script which installs appropriate git hooks for testing etc. Notes As far as possible, we follow the Google Python Style Guide As for Git practices, please follow the steps described at Swiss Cheese for how to git-rebase-squash when working on a forked repo.","title":"Contribute"},{"location":"contribute/#contribute","text":"","title":"Contribute"},{"location":"contribute/#dev-install","text":"After downloading the code repository (via git clone ), change to the repository directory ( cd neural_prophet ), activate your virtual environment, and install neuralprophet as python package with pip install -e .[dev] (Including the optional -e flag will install neuralprophet in \"editable\" mode, meaning that instead of copying the files into your virtual environment, a symlink will be created to the files where they are.) Additionally you must run $ neuralprophet_dev_setup in your console to run the dev-setup script which installs appropriate git hooks for testing etc.","title":"Dev Install"},{"location":"contribute/#notes","text":"As far as possible, we follow the Google Python Style Guide As for Git practices, please follow the steps described at Swiss Cheese for how to git-rebase-squash when working on a forked repo.","title":"Notes"},{"location":"hyperparameter-selection/","text":"Selecting the Hyperparameters NeuralProphet has a number of hyperparameters that need to be specified by the user. If not specified, default values for these hyperparameters will be used. They are as follows. Parameter Default Value growth linear changepoints None n_changepoints 5 changepoints_range 0.8 trend_reg 0 trend_reg_threshold False yearly_seasonality auto weekly_seasonality auto daily_seasonality auto seasonality_mode additive seasonality_reg None n_forecasts 1 n_lags 0 num_hidden_layers 0 d_hidden None ar_sparsity None learning_rate None epochs 40 loss_func Huber normalize_y auto impute_missing True log_level None Forecast horizon n_forecasts is the size of the forecast horizon. The default value of 1 means that the model forecasts one step into the future. Autoregression n_lags defines whether the AR-Net is enabled (if n_lags > 0) or not. The value for n_lags is usually recommended to be greater than n_forecasts , if possible since it is preferable for the FFNNs to encounter at least n_forecasts length of the past in order to predict n_forecasts into the future. Thus, n_lags determine how far into the past the auto-regressive dependencies should be considered. This could be a value chosen based on either domain expertise or an empirical analysis. Model Training Related Parameters NeuralProphet is fit with stochastic gradient descent - more precisely, with an AdamW optimizer and a One-Cycle policy. If the parameter learning_rate is not specified, a learning rate range test is conducted to determin the optimal learning rate. The epochs and the loss_func are two other parameters that directly affect the model training process. If, by looking at the live loss plot, the model looks like it's underfitting, the number of epochs or the learning_rate can be increased. On the other hand, if the model looks like it is overfitting to the training data, the number of epochs or the learning_rate can be reduced. The default for epochs is 40, which could likely be reduced for most datasets. The default loss function is Huber loss, which is considered to be robust to outliers. However, you can select from Huber , MAE and MSE or any PyTorch loss function. Increasing Depth of the Model num_hidden_layers defines the number of hidden layers of the FFNNs used in the overall model. This includes the AR-Net and the FFNN of the lagged regressors. The default is 0, meaning that the FFNNs will have only one final layer of size n_forecasts . Adding more layers results in increased complexity and also increased computational time, consequently. However, the added number of hidden layers can help build more complex relationships especially useful for the lagged regressors. To tradeoff between the computational complexity and the improved accuracy the num_hidden_layers is recommended to be set in between 1-2. Nevertheless, in most cases a good enough performance can be achieved by having no hidden layers at all. d_hidden is the number of units in the hidden layers. This is only considered if num_hidden_layers is specified, otherwise ignored. The default value for d_hidden if not specified is ( n_lags + n_forecasts ). If tuned manually, the recommended practice is to set a value in between n_lags and n_forecasts for d_hidden . It is also important to note that with the current implementation, NeuralProphet sets the same d_hidden for the all the hidden layers. Data Preprocessing Related Parameters normalize_y is about scaling the time series before modelling. By default, NeuralProphet performs a (soft) min-max normalization of the time series. Normalization can help the model training process if the series values fluctuate heavily. However, if the series does not such scaling, users can turn this off or select another normalization. impute_missing is about imputing the missing values in a given series. S imilar to Prophet, NeuralProphet too can work with missing values when it is in the regression mode without the AR-Net. However, when the autocorrelation needs to be captured, it is necessary for the missing values to be imputed, since then the modelling becomes an ordered problem. Letting this parameter at its default can get the job done perfectly in most cases. Trend Related Parameters You can find a hands-on example at example_notebooks/trend_peyton_manning.ipynb . The trend flexibility if primarily controlled by n_changepoints , which sets the number of points where the trend rate may change. Additionally, the trend rate changes can be regularized by setting trend_reg to a value greater zero. This is a useful feature that can be used to automatically detect relevant changepoints. changepoints_range controls the range of training data used to fit the trend. The default value of 0.8 means that no changepoints are set in the last 20 percent of training data. If a list of changepoints is supplied, n_changepoints and changepoints_range are ignored. This list is instead used to set the dates at which the trend rate is allowed to change. n_changepoints is the number of changepoints selected along the series for the trend. The default value for this is 5. Seasonality Related Parameters yearly_seasonality , weekly_seasonality and daily_seasonality are about which seasonal components to be modelled. For example, if you use temperature data, you can probably select daily and yearly. Using number of passengers using the subway would more likely have a weekly seasonality for example. Setting these seasonalities at the default auto mode, lets NeuralProphet decide which of them to include depending on how much data available. For example, the yearly seasonality will not be considered if less than two years data available. In the same manner, the weekly seasonality will not be considered if less than two weeks available etc... However, if the user if certain that the series does not include yearly, weekly or daily seasonality, and thus the model should not be distorted by such components, they can explicitly turn them off by setting the respective components to False . Apart from that, the parameters yearly_seasonality , weekly_seasonality and daily_seasonality can also be set to number of Fourier terms of the respective seasonalities. The defaults are 6 for yearly, 4 for weekly and 6 for daily. Users can set this to any number they want. If the number of terms is 6 for yearly, that effectively makes the total number of Fourier terms for the yearly seasonality 12 (6*2), to accommodate both sine and cosine terms. Increasing the number of Fourier terms can make the model capable of capturing quite complex seasonal patterns. However, similar to the num_hidden_layers , this too results in added model complexity. Users can get some insights about the optimal number of Fourier terms by looking at the final component plots. The default seasonality_mode is additive. This means that no heteroscedasticity is expected in the series in terms of the seasonality. However, if the series contains clear variance, where the seasonal fluctuations become larger proportional to the trend, the seasonality_mode can be set to multiplicative. Regularization Related Parameters NeuralProphet also contains a number of regularization parameters to control the model coefficients and introduce sparsity into the model. This also helps avoid overfitting of the model to the training data. For seasonality_reg , small values in the range 0.1-1 allow to fit large seasonal fluctuations whereas large values in the range 1-100 impose a heavier penalty on the Fourier coefficients and thus dampens the seasonality. For ar_sparsity values in the range 0-1 are expected with 0 inducing complete sparsity and 1 imposing no regularization at all. ar_sparsity along with n_lags can be used for data exploration and feature selection. You can use a larger number of lags thanks to the scalability of AR-Net and use the scarcity to identify important influence of past time steps on the prediction accuracy. For future_regressor_regularization , event_regularization and country_holiday_regularization , values can be set in between 0-1 in the same notion as in ar_sparsity . You can set different regularization parameters for the individual regressors and events depending on which ones need to be more dampened.","title":"Hyperparameter Selection"},{"location":"hyperparameter-selection/#selecting-the-hyperparameters","text":"NeuralProphet has a number of hyperparameters that need to be specified by the user. If not specified, default values for these hyperparameters will be used. They are as follows. Parameter Default Value growth linear changepoints None n_changepoints 5 changepoints_range 0.8 trend_reg 0 trend_reg_threshold False yearly_seasonality auto weekly_seasonality auto daily_seasonality auto seasonality_mode additive seasonality_reg None n_forecasts 1 n_lags 0 num_hidden_layers 0 d_hidden None ar_sparsity None learning_rate None epochs 40 loss_func Huber normalize_y auto impute_missing True log_level None","title":"Selecting the Hyperparameters"},{"location":"hyperparameter-selection/#forecast-horizon","text":"n_forecasts is the size of the forecast horizon. The default value of 1 means that the model forecasts one step into the future.","title":"Forecast horizon"},{"location":"hyperparameter-selection/#autoregression","text":"n_lags defines whether the AR-Net is enabled (if n_lags > 0) or not. The value for n_lags is usually recommended to be greater than n_forecasts , if possible since it is preferable for the FFNNs to encounter at least n_forecasts length of the past in order to predict n_forecasts into the future. Thus, n_lags determine how far into the past the auto-regressive dependencies should be considered. This could be a value chosen based on either domain expertise or an empirical analysis.","title":"Autoregression"},{"location":"hyperparameter-selection/#model-training-related-parameters","text":"NeuralProphet is fit with stochastic gradient descent - more precisely, with an AdamW optimizer and a One-Cycle policy. If the parameter learning_rate is not specified, a learning rate range test is conducted to determin the optimal learning rate. The epochs and the loss_func are two other parameters that directly affect the model training process. If, by looking at the live loss plot, the model looks like it's underfitting, the number of epochs or the learning_rate can be increased. On the other hand, if the model looks like it is overfitting to the training data, the number of epochs or the learning_rate can be reduced. The default for epochs is 40, which could likely be reduced for most datasets. The default loss function is Huber loss, which is considered to be robust to outliers. However, you can select from Huber , MAE and MSE or any PyTorch loss function.","title":"Model Training Related Parameters"},{"location":"hyperparameter-selection/#increasing-depth-of-the-model","text":"num_hidden_layers defines the number of hidden layers of the FFNNs used in the overall model. This includes the AR-Net and the FFNN of the lagged regressors. The default is 0, meaning that the FFNNs will have only one final layer of size n_forecasts . Adding more layers results in increased complexity and also increased computational time, consequently. However, the added number of hidden layers can help build more complex relationships especially useful for the lagged regressors. To tradeoff between the computational complexity and the improved accuracy the num_hidden_layers is recommended to be set in between 1-2. Nevertheless, in most cases a good enough performance can be achieved by having no hidden layers at all. d_hidden is the number of units in the hidden layers. This is only considered if num_hidden_layers is specified, otherwise ignored. The default value for d_hidden if not specified is ( n_lags + n_forecasts ). If tuned manually, the recommended practice is to set a value in between n_lags and n_forecasts for d_hidden . It is also important to note that with the current implementation, NeuralProphet sets the same d_hidden for the all the hidden layers.","title":"Increasing Depth of the Model"},{"location":"hyperparameter-selection/#data-preprocessing-related-parameters","text":"normalize_y is about scaling the time series before modelling. By default, NeuralProphet performs a (soft) min-max normalization of the time series. Normalization can help the model training process if the series values fluctuate heavily. However, if the series does not such scaling, users can turn this off or select another normalization. impute_missing is about imputing the missing values in a given series. S imilar to Prophet, NeuralProphet too can work with missing values when it is in the regression mode without the AR-Net. However, when the autocorrelation needs to be captured, it is necessary for the missing values to be imputed, since then the modelling becomes an ordered problem. Letting this parameter at its default can get the job done perfectly in most cases.","title":"Data Preprocessing Related Parameters"},{"location":"hyperparameter-selection/#trend-related-parameters","text":"You can find a hands-on example at example_notebooks/trend_peyton_manning.ipynb . The trend flexibility if primarily controlled by n_changepoints , which sets the number of points where the trend rate may change. Additionally, the trend rate changes can be regularized by setting trend_reg to a value greater zero. This is a useful feature that can be used to automatically detect relevant changepoints. changepoints_range controls the range of training data used to fit the trend. The default value of 0.8 means that no changepoints are set in the last 20 percent of training data. If a list of changepoints is supplied, n_changepoints and changepoints_range are ignored. This list is instead used to set the dates at which the trend rate is allowed to change. n_changepoints is the number of changepoints selected along the series for the trend. The default value for this is 5.","title":"Trend Related Parameters"},{"location":"hyperparameter-selection/#seasonality-related-parameters","text":"yearly_seasonality , weekly_seasonality and daily_seasonality are about which seasonal components to be modelled. For example, if you use temperature data, you can probably select daily and yearly. Using number of passengers using the subway would more likely have a weekly seasonality for example. Setting these seasonalities at the default auto mode, lets NeuralProphet decide which of them to include depending on how much data available. For example, the yearly seasonality will not be considered if less than two years data available. In the same manner, the weekly seasonality will not be considered if less than two weeks available etc... However, if the user if certain that the series does not include yearly, weekly or daily seasonality, and thus the model should not be distorted by such components, they can explicitly turn them off by setting the respective components to False . Apart from that, the parameters yearly_seasonality , weekly_seasonality and daily_seasonality can also be set to number of Fourier terms of the respective seasonalities. The defaults are 6 for yearly, 4 for weekly and 6 for daily. Users can set this to any number they want. If the number of terms is 6 for yearly, that effectively makes the total number of Fourier terms for the yearly seasonality 12 (6*2), to accommodate both sine and cosine terms. Increasing the number of Fourier terms can make the model capable of capturing quite complex seasonal patterns. However, similar to the num_hidden_layers , this too results in added model complexity. Users can get some insights about the optimal number of Fourier terms by looking at the final component plots. The default seasonality_mode is additive. This means that no heteroscedasticity is expected in the series in terms of the seasonality. However, if the series contains clear variance, where the seasonal fluctuations become larger proportional to the trend, the seasonality_mode can be set to multiplicative.","title":"Seasonality Related Parameters"},{"location":"hyperparameter-selection/#regularization-related-parameters","text":"NeuralProphet also contains a number of regularization parameters to control the model coefficients and introduce sparsity into the model. This also helps avoid overfitting of the model to the training data. For seasonality_reg , small values in the range 0.1-1 allow to fit large seasonal fluctuations whereas large values in the range 1-100 impose a heavier penalty on the Fourier coefficients and thus dampens the seasonality. For ar_sparsity values in the range 0-1 are expected with 0 inducing complete sparsity and 1 imposing no regularization at all. ar_sparsity along with n_lags can be used for data exploration and feature selection. You can use a larger number of lags thanks to the scalability of AR-Net and use the scarcity to identify important influence of past time steps on the prediction accuracy. For future_regressor_regularization , event_regularization and country_holiday_regularization , values can be set in between 0-1 in the same notion as in ar_sparsity . You can set different regularization parameters for the individual regressors and events depending on which ones need to be more dampened.","title":"Regularization Related Parameters"},{"location":"model-overview/","text":"The documentation is a work in progress. Overview of the NeuralProphet Model NeuralProphet is a Neural Network based PyTorch implementation of a user-friendly time series forecasting tool for practitioners. This is heavily inspired by Prophet , which is the popular forecasting tool developed by Facebook. NeuralProphet is developed in a fully modular architecture which makes it scalable to add any additional components in the future. Our vision is to develop a simple to use forecasting tool for users while retaining the original objectives of Prophet such as interpretability, configurability and providing much more such as the automatic differencing capabilities by using PyTorch as the backend. Time Series Components NeuralProphet is a decomposable time series model with the components, trend, seasonality, auto-regression, special events, future regressors and lagged regressors. Future regressors are external variables which have known future values for the forecast period whereas the lagged regressors are those external variables which only have values for the observed period. Trend can be modelled either as a linear or a piece-wise linear trend by using changepoints. Seasonality is modelled using fourier terms and thus can handle multiple seasonalities for high-frequency data. Auto-regression is handled using an implementation of AR-Net , an Auto-Regressive Feed-Forward Neural Network for time series. Lagged regressors are also modelled using separate Feed-Forward Neural Networks. Future regressors and special events are both modelled as covariates of the model with dedicated coefficients. For more details, refer to the documentation of the individual components. Data Preprocessing We perform a few data pre-processing steps in the model. For the observed values of the time series, users can specify whether they would like the values to be normalized. By default, the y values would be min-max normalized. If the user specifically, sets the normalize_y argument to true , the data is z-score normalized. Normalization can be performed for covariates as well. The default mode for normalization of covariates is auto . In this mode, apart from binary features such as events, all others are z-score normalized. We also perform an imputation in-case there are missing values in the data. However, imputation is only done if auto-regression is enabled in the model. Otherwise, the missing values do not really matter for the regression model. No special imputation is done for binary data. They are simply taken as 0 for the missing dates. For the numeric data, including the y values, normalization is a two-step process. First, small gaps are filled with a linear imputation and then the more larger gaps are filled with rolling averages. When auto-regression is enabled, the observed y values are preprocessed in a moving window format to learn from lagged values. This is done for lagged regressors as well. When to Use NeuralProphet NeuralProphet can produce both single step and multi step-ahead forecasts. At the moment, NeuralProphet builds models univariately. This means that if you have many series that you expect to produce forecasts for, you need to do this one at a time. However, in future we hope to integrate the capability of global forecasting models into NeuralProphet. NeuralProphet helps build forecasting models for scenarios where there are other external factors which can drive the behaviour of the target series over time. Using such external information can heavily improve forecasting models rather than relying only on the autocorrelation of the series. NeuralProphet tool is suitable for forecasting practitioners that wish to gain insights into the overall modelling process by visualizing the forecasts, the individual components as well as the underlying coefficients of the model. Through our descriptive plots, users can visualize the interaction of the individual components. They also have the power to control these coefficients as required by introducing sparsity through regularization. They can combine the components additively or multiplicatively as per their domain knowledge. This is an ongoing effort. Therefore, NeuralProphet will be equipped with even much more features in the upcoming releases.","title":"Model Overview"},{"location":"model-overview/#overview-of-the-neuralprophet-model","text":"NeuralProphet is a Neural Network based PyTorch implementation of a user-friendly time series forecasting tool for practitioners. This is heavily inspired by Prophet , which is the popular forecasting tool developed by Facebook. NeuralProphet is developed in a fully modular architecture which makes it scalable to add any additional components in the future. Our vision is to develop a simple to use forecasting tool for users while retaining the original objectives of Prophet such as interpretability, configurability and providing much more such as the automatic differencing capabilities by using PyTorch as the backend.","title":"Overview of the NeuralProphet Model"},{"location":"model-overview/#time-series-components","text":"NeuralProphet is a decomposable time series model with the components, trend, seasonality, auto-regression, special events, future regressors and lagged regressors. Future regressors are external variables which have known future values for the forecast period whereas the lagged regressors are those external variables which only have values for the observed period. Trend can be modelled either as a linear or a piece-wise linear trend by using changepoints. Seasonality is modelled using fourier terms and thus can handle multiple seasonalities for high-frequency data. Auto-regression is handled using an implementation of AR-Net , an Auto-Regressive Feed-Forward Neural Network for time series. Lagged regressors are also modelled using separate Feed-Forward Neural Networks. Future regressors and special events are both modelled as covariates of the model with dedicated coefficients. For more details, refer to the documentation of the individual components.","title":"Time Series Components"},{"location":"model-overview/#data-preprocessing","text":"We perform a few data pre-processing steps in the model. For the observed values of the time series, users can specify whether they would like the values to be normalized. By default, the y values would be min-max normalized. If the user specifically, sets the normalize_y argument to true , the data is z-score normalized. Normalization can be performed for covariates as well. The default mode for normalization of covariates is auto . In this mode, apart from binary features such as events, all others are z-score normalized. We also perform an imputation in-case there are missing values in the data. However, imputation is only done if auto-regression is enabled in the model. Otherwise, the missing values do not really matter for the regression model. No special imputation is done for binary data. They are simply taken as 0 for the missing dates. For the numeric data, including the y values, normalization is a two-step process. First, small gaps are filled with a linear imputation and then the more larger gaps are filled with rolling averages. When auto-regression is enabled, the observed y values are preprocessed in a moving window format to learn from lagged values. This is done for lagged regressors as well.","title":"Data Preprocessing"},{"location":"model-overview/#when-to-use-neuralprophet","text":"NeuralProphet can produce both single step and multi step-ahead forecasts. At the moment, NeuralProphet builds models univariately. This means that if you have many series that you expect to produce forecasts for, you need to do this one at a time. However, in future we hope to integrate the capability of global forecasting models into NeuralProphet. NeuralProphet helps build forecasting models for scenarios where there are other external factors which can drive the behaviour of the target series over time. Using such external information can heavily improve forecasting models rather than relying only on the autocorrelation of the series. NeuralProphet tool is suitable for forecasting practitioners that wish to gain insights into the overall modelling process by visualizing the forecasts, the individual components as well as the underlying coefficients of the model. Through our descriptive plots, users can visualize the interaction of the individual components. They also have the power to control these coefficients as required by introducing sparsity through regularization. They can combine the components additively or multiplicatively as per their domain knowledge. This is an ongoing effort. Therefore, NeuralProphet will be equipped with even much more features in the upcoming releases.","title":"When to Use NeuralProphet"},{"location":"model/auto-regression/","text":"Modelling Auto-Regression AR-Net can be enabled in the NeuralProphet by simply setting an appropriate value to the n_lags parameter of the NeuralProphet object. m = NeuralProphet( n_forecasts=3, n_lags=5, yearly_seasonality=False, weekly_seasonality=False, daily_seasonality=False, ) In the above example, we create a forecasting scenario which feeds 5 lags into AR-Net and receives 3 steps as forecasts. Once you have the AR-Net enabled, during forecasting your future_periods value should be equal to the n_forecasts value specified when creating the NeuralProphet object. Whichever value you specify for future_periods , it will be converted to the value of n_forecasts with a notice to the user. This is because, since the AR-Net is built during training such that it has an ouptut size of n_forecasts , it cannot support any other value during testing. The plotted components should look like below. You can now see auto-regression as a separate component. The corresponding coefficients look like below. You can see the relevance of each of the lags when modelling the autocorrelation. You can also specify the num_hidden_layers for the AR-Net, in order to increase the complexity of the AR-Net. m = NeuralProphet( n_forecasts=3, n_lags=5, num_hidden_layers=2, yearly_seasonality=False, weekly_seasonality=False, daily_seasonality=False ) Regularize AR-Net Regularization in AR-Net is done by setting the ar_sparsity parameter in the NeuralProphet object like below. For more details on setting a value for ar_sparsity , refer to the Section on Hyperparameter Selection . m = NeuralProphet( n_forecasts=3, n_lags=5, num_hidden_layers=2, ar_sparsity=0.01, yearly_seasonality=False, weekly_seasonality=False, daily_seasonality=False ) Highlight Specific Forecast Step When modelling the autocorrelation, the model in the multi-input, multi-output mode. In this mode, you can highlight the nth step ahead forecast. This means that, you can specifically look at the forecast at the nth step when calculating errors during model training as well as when forecast plotting. This can be done like below. m = NeuralProphet( n_forecasts=30, n_lags=60, yearly_seasonality=False, weekly_seasonality=False, daily_seasonality=False ) m.highlight_nth_step_ahead_of_each_forecast(step_number=m.n_forecasts) You can specify any value less than or equal to n_forecasts to the step_number parameter. Once you do this, the metrics would look like below. SmoothL1Loss MAE SmoothL1Loss-3 MAE-3 RegLoss 0.272427 3.063127 0.164296 2.407697 0.0 0.151259 2.303768 0.144811 2.261525 0.0 0.129990 2.140769 0.127703 2.126293 0.0 0.116178 2.020397 0.113719 2.005068 0.0 0.104502 1.915078 0.101155 1.887193 0.0 In the forecast plots, it will focus only on the nth step ahead forecast. This is shown below for the fir of the model.","title":"Auto-Regression"},{"location":"model/auto-regression/#modelling-auto-regression","text":"AR-Net can be enabled in the NeuralProphet by simply setting an appropriate value to the n_lags parameter of the NeuralProphet object. m = NeuralProphet( n_forecasts=3, n_lags=5, yearly_seasonality=False, weekly_seasonality=False, daily_seasonality=False, ) In the above example, we create a forecasting scenario which feeds 5 lags into AR-Net and receives 3 steps as forecasts. Once you have the AR-Net enabled, during forecasting your future_periods value should be equal to the n_forecasts value specified when creating the NeuralProphet object. Whichever value you specify for future_periods , it will be converted to the value of n_forecasts with a notice to the user. This is because, since the AR-Net is built during training such that it has an ouptut size of n_forecasts , it cannot support any other value during testing. The plotted components should look like below. You can now see auto-regression as a separate component. The corresponding coefficients look like below. You can see the relevance of each of the lags when modelling the autocorrelation. You can also specify the num_hidden_layers for the AR-Net, in order to increase the complexity of the AR-Net. m = NeuralProphet( n_forecasts=3, n_lags=5, num_hidden_layers=2, yearly_seasonality=False, weekly_seasonality=False, daily_seasonality=False )","title":"Modelling Auto-Regression"},{"location":"model/auto-regression/#regularize-ar-net","text":"Regularization in AR-Net is done by setting the ar_sparsity parameter in the NeuralProphet object like below. For more details on setting a value for ar_sparsity , refer to the Section on Hyperparameter Selection . m = NeuralProphet( n_forecasts=3, n_lags=5, num_hidden_layers=2, ar_sparsity=0.01, yearly_seasonality=False, weekly_seasonality=False, daily_seasonality=False )","title":"Regularize AR-Net"},{"location":"model/auto-regression/#highlight-specific-forecast-step","text":"When modelling the autocorrelation, the model in the multi-input, multi-output mode. In this mode, you can highlight the nth step ahead forecast. This means that, you can specifically look at the forecast at the nth step when calculating errors during model training as well as when forecast plotting. This can be done like below. m = NeuralProphet( n_forecasts=30, n_lags=60, yearly_seasonality=False, weekly_seasonality=False, daily_seasonality=False ) m.highlight_nth_step_ahead_of_each_forecast(step_number=m.n_forecasts) You can specify any value less than or equal to n_forecasts to the step_number parameter. Once you do this, the metrics would look like below. SmoothL1Loss MAE SmoothL1Loss-3 MAE-3 RegLoss 0.272427 3.063127 0.164296 2.407697 0.0 0.151259 2.303768 0.144811 2.261525 0.0 0.129990 2.140769 0.127703 2.126293 0.0 0.116178 2.020397 0.113719 2.005068 0.0 0.104502 1.915078 0.101155 1.887193 0.0 In the forecast plots, it will focus only on the nth step ahead forecast. This is shown below for the fir of the model.","title":"Highlight Specific Forecast Step"},{"location":"model/events/","text":"Modelling Events Often in forecasting problems, we need to consider recurring special events. These are supported by neural_prophet . These events can be added both in additive format and multiplicative format. To provide the information of events into the model, the user has to create a dataframe which has the column ds corresponding to the event dates and the column event which contains the names of the events on specified dates. In the following example we have created the dataframe named history_events_df which contains these events information. playoffs_history = pd.DataFrame({ 'event': 'playoff', 'ds': pd.to_datetime(['2008-01-13', '2009-01-03', '2010-01-16', '2010-01-24', '2010-02-07', '2011-01-08', '2013-01-12', '2014-01-12', '2014-01-19', '2014-02-02', '2015-01-11', '2016-01-17']), }) superbowls_history = pd.DataFrame({ 'event': 'superbowl', 'ds': pd.to_datetime(['2010-02-07', '2014-02-02']), }) history_events_df = pd.concat((playoffs_history, superbowls_history)) The first few rows of the history_events_df dataframe looks like below. event ds 0 playoff 2008-01-13 00:00:00 1 playoff 2009-01-03 00:00:00 2 playoff 2010-01-16 00:00:00 3 playoff 2010-01-24 00:00:00 4 playoff 2010-02-07 00:00:00 5 playoff 2011-01-08 00:00:00 For forecasting, we also need to provide the future dates of these events used to train the model. You can either include these in the same events dataframe that was created before for fitting the model, or in a new dataframe as follows. playoffs_future = pd.DataFrame({ 'event': 'playoff', 'ds': pd.to_datetime(['2016-01-21', '2016-02-07']) }) superbowl_future = pd.DataFrame({ 'event': 'superbowl', 'ds': pd.to_datetime(['2016-01-23', '2016-02-07']) }) future_events_df = pd.concat((playoffs_future, superbowl_future)) Once the events dataframes have been created, the NeuralProphet object should be created and the events configs should be added. This is done using the add_events function of the NeuralProphet class. m = NeuralProphet( n_forecasts=10, yearly_seasonality=False, weekly_seasonality=False, daily_seasonality=False, ) m = m.add_events([\"superbowl\", \"playoff\"]) After that, we need to convert the events data in the previously created dataframes into the binary input data expected by the model. This can be done by calling the create_df_with_events function by passing original time series dataframe along with the created history_events_df . history_df = m.create_df_with_events(df, history_events_df) This returns a dataframe in the following format. ds y superbowl playoff 0 2007-12-10 00:00:00 9.59076 0 0 1 2007-12-11 00:00:00 8.51959 0 0 2 2007-12-12 00:00:00 8.18368 0 0 3 2007-12-13 00:00:00 8.07247 0 0 4 2007-12-14 00:00:00 7.89357 0 0 After that, we can simply fit the model as below by providing to the fit function, the created history_df . metrics = m.fit(history_df, freq=\"D\") To do forecasting with the fitted model, we first need to create the future dataframe with events information. This can be done with the make_future_dataframe function by passing in the created future_events_df and specifying the desired size of the forecast horizon. future = m.make_future_dataframe(df=history_df, events_df=future_events_df, periods=10) forecast = m.predict(df=future) Once the forecasting is done, the different components can be plotted like below. All events are plotted as one component, the Additive Events The model coefficients would look like below. Multiplicative Events The default mode for events in neural_prophet is additive. However, events can also be modelled in a multiplicative format. For this, when adding the events configs to the NeuralProphet object, we need to set the mode to multiplicative as below. m = m.add_events([\"superbowl\", \"playoff\"], mode=\"multiplicative\") All the other steps are the same as for the additive mode. Now, when you plot the components, the event components will appear as percentages. Event Windows You can also provide windows for events. This way, you can consider the days around a particular event also as special events by providing the arguments lower_window and upper_window as appropriate to the add_events function of the NeuralProphet object. By default, the values for these windows are 0 , which means windows are not considered. m = m.add_events([\"superbowl\", \"playoff\"], lower_window=-1, upper_window=1) According to this specification, for both superbowl and playoff events, three special events will be modelled, the event date, the previous day and the next day. These will be visible in the component plots as below. In the parameters plot too, there will now be superbowl_+1 and superbowl_-1 which correspond to the coefficients of the day following and previous to the superbowl event. The playoff event also has the same new coefficients. If you want to define different windows for the individual events, this can also be done as follows. m = m.add_events(\"superbowl\", lower_window=-1, upper_window=1) m = m.add_events(\"playoff\", upper_window=2) In the above example, for the playoff event, the specified event date, as well as the two following dates are considered as three different special events. Country Specific Holidays Apart from the user specified events, neural_prophet also supports standard country specific holidays. If you want to add the holidays for a particular country, you simply have to call the add_country_holidays function on the NeuralProphet object and specify the country. Similar to the user specified events, country specific holidays can either be additive or multiplicative and include windows. However, unlike for user specified events, the windows will be the same for all the country specific events. m = m.add_country_holidays(\"US\", mode=\"additive\", lower_window=-1, upper_window=1) This example will add all the US holidays into the model in additive format. The coefficients of the individual events will now look like below. Regularization for Events Events can also support regularization of the coefficients. You can specify the regularization when adding the event configs into the NeuralProphet object like below. m = m.add_events([\"superbowl\", \"playoff\"], regularization=0.05) The regularization for the individual events can also be different from each other like below. m = m.add_events(\"superbowl\", regularization=0.05) m = m.add_events(\"playoff\", regularization=0.03) For the country specific holidays too, regularizations can be specified like below. m = m.add_country_holidays(\"US\", mode=\"additive\", regularization=0.05)","title":"Events"},{"location":"model/events/#modelling-events","text":"Often in forecasting problems, we need to consider recurring special events. These are supported by neural_prophet . These events can be added both in additive format and multiplicative format. To provide the information of events into the model, the user has to create a dataframe which has the column ds corresponding to the event dates and the column event which contains the names of the events on specified dates. In the following example we have created the dataframe named history_events_df which contains these events information. playoffs_history = pd.DataFrame({ 'event': 'playoff', 'ds': pd.to_datetime(['2008-01-13', '2009-01-03', '2010-01-16', '2010-01-24', '2010-02-07', '2011-01-08', '2013-01-12', '2014-01-12', '2014-01-19', '2014-02-02', '2015-01-11', '2016-01-17']), }) superbowls_history = pd.DataFrame({ 'event': 'superbowl', 'ds': pd.to_datetime(['2010-02-07', '2014-02-02']), }) history_events_df = pd.concat((playoffs_history, superbowls_history)) The first few rows of the history_events_df dataframe looks like below. event ds 0 playoff 2008-01-13 00:00:00 1 playoff 2009-01-03 00:00:00 2 playoff 2010-01-16 00:00:00 3 playoff 2010-01-24 00:00:00 4 playoff 2010-02-07 00:00:00 5 playoff 2011-01-08 00:00:00 For forecasting, we also need to provide the future dates of these events used to train the model. You can either include these in the same events dataframe that was created before for fitting the model, or in a new dataframe as follows. playoffs_future = pd.DataFrame({ 'event': 'playoff', 'ds': pd.to_datetime(['2016-01-21', '2016-02-07']) }) superbowl_future = pd.DataFrame({ 'event': 'superbowl', 'ds': pd.to_datetime(['2016-01-23', '2016-02-07']) }) future_events_df = pd.concat((playoffs_future, superbowl_future)) Once the events dataframes have been created, the NeuralProphet object should be created and the events configs should be added. This is done using the add_events function of the NeuralProphet class. m = NeuralProphet( n_forecasts=10, yearly_seasonality=False, weekly_seasonality=False, daily_seasonality=False, ) m = m.add_events([\"superbowl\", \"playoff\"]) After that, we need to convert the events data in the previously created dataframes into the binary input data expected by the model. This can be done by calling the create_df_with_events function by passing original time series dataframe along with the created history_events_df . history_df = m.create_df_with_events(df, history_events_df) This returns a dataframe in the following format. ds y superbowl playoff 0 2007-12-10 00:00:00 9.59076 0 0 1 2007-12-11 00:00:00 8.51959 0 0 2 2007-12-12 00:00:00 8.18368 0 0 3 2007-12-13 00:00:00 8.07247 0 0 4 2007-12-14 00:00:00 7.89357 0 0 After that, we can simply fit the model as below by providing to the fit function, the created history_df . metrics = m.fit(history_df, freq=\"D\") To do forecasting with the fitted model, we first need to create the future dataframe with events information. This can be done with the make_future_dataframe function by passing in the created future_events_df and specifying the desired size of the forecast horizon. future = m.make_future_dataframe(df=history_df, events_df=future_events_df, periods=10) forecast = m.predict(df=future) Once the forecasting is done, the different components can be plotted like below. All events are plotted as one component, the Additive Events The model coefficients would look like below.","title":"Modelling Events"},{"location":"model/events/#multiplicative-events","text":"The default mode for events in neural_prophet is additive. However, events can also be modelled in a multiplicative format. For this, when adding the events configs to the NeuralProphet object, we need to set the mode to multiplicative as below. m = m.add_events([\"superbowl\", \"playoff\"], mode=\"multiplicative\") All the other steps are the same as for the additive mode. Now, when you plot the components, the event components will appear as percentages.","title":"Multiplicative Events"},{"location":"model/events/#event-windows","text":"You can also provide windows for events. This way, you can consider the days around a particular event also as special events by providing the arguments lower_window and upper_window as appropriate to the add_events function of the NeuralProphet object. By default, the values for these windows are 0 , which means windows are not considered. m = m.add_events([\"superbowl\", \"playoff\"], lower_window=-1, upper_window=1) According to this specification, for both superbowl and playoff events, three special events will be modelled, the event date, the previous day and the next day. These will be visible in the component plots as below. In the parameters plot too, there will now be superbowl_+1 and superbowl_-1 which correspond to the coefficients of the day following and previous to the superbowl event. The playoff event also has the same new coefficients. If you want to define different windows for the individual events, this can also be done as follows. m = m.add_events(\"superbowl\", lower_window=-1, upper_window=1) m = m.add_events(\"playoff\", upper_window=2) In the above example, for the playoff event, the specified event date, as well as the two following dates are considered as three different special events.","title":"Event Windows"},{"location":"model/events/#country-specific-holidays","text":"Apart from the user specified events, neural_prophet also supports standard country specific holidays. If you want to add the holidays for a particular country, you simply have to call the add_country_holidays function on the NeuralProphet object and specify the country. Similar to the user specified events, country specific holidays can either be additive or multiplicative and include windows. However, unlike for user specified events, the windows will be the same for all the country specific events. m = m.add_country_holidays(\"US\", mode=\"additive\", lower_window=-1, upper_window=1) This example will add all the US holidays into the model in additive format. The coefficients of the individual events will now look like below.","title":"Country Specific Holidays"},{"location":"model/events/#regularization-for-events","text":"Events can also support regularization of the coefficients. You can specify the regularization when adding the event configs into the NeuralProphet object like below. m = m.add_events([\"superbowl\", \"playoff\"], regularization=0.05) The regularization for the individual events can also be different from each other like below. m = m.add_events(\"superbowl\", regularization=0.05) m = m.add_events(\"playoff\", regularization=0.03) For the country specific holidays too, regularizations can be specified like below. m = m.add_country_holidays(\"US\", mode=\"additive\", regularization=0.05)","title":"Regularization for Events"},{"location":"model/future-regressors/","text":"Modelling Future Regressors Future regressors are the external variables which have known future values. In that sense, the future regressors functionality if very similar to special events. The past values of these regressors corresponding to the training time stamps, have to be provided along with the training data itself. See below for an example where we create two dummy regressors A and B by taking rolling means of the original data. df['A'] = df['y'].rolling(7, min_periods=1).mean() df['B'] = df['y'].rolling(30, min_periods=1).mean() The dataframe created likewise, should look like below. ds y A B 0 2007-12-10 9.59076 9.59076 9.59076 1 2007-12-11 8.51959 9.05518 9.05518 2 2007-12-12 8.18368 8.76468 8.76468 3 2007-12-13 8.07247 8.59162 8.59162 4 2007-12-14 7.89357 8.45201 8.45201 In order to perform forecasting, we also need to provide the future values of the regressors. future_regressors_df = pd.DataFrame(data={'A': df['A'][:50], 'B': df['B'][:50]}) This dataframe looks like below. A B 0 9.59076 9.59076 1 9.05518 9.05518 2 8.76468 8.76468 3 8.59162 8.59162 4 8.45201 8.45201 It is a dataframe with only the columns of the future values of the regressors. Similar to events, future regressors too can be added in both the additive and multiplicative formats. Additive Future Regressors The default mode for future regressors in neural_prophet is additive. The regressors have to be added to the NeuralProphet object by calling the add_future_regressor function. Once this is done, the model can be fitted by providing to the fit function, the dataframe of the training data as well as the regressor values. m = NeuralProphet( n_forecasts=10, yearly_seasonality=False, weekly_seasonality=False, daily_seasonality=False, ) m = m.add_future_regressor(name='A') m = m.add_future_regressor(name='B') metrics = m.fit(df, freq=\"D\") When forecasting, the future dataframe must be created by providing the future values of the regressors. To do that, now you need to call the make_future_dataframe function by providing the previously created future_regressors_df as an argument. future = m.make_future_dataframe(df=df, regressors_df=future_regressors_df, periods=3) forecast = m.predict(df=future) Now you can plot the components the same way as before and the resulting plot would look something like below. fig_comp = m.plot_components(forecast) In addition to the trend it also shows a plot for the additive future regressors. The coefficients of the future regressors can also be plotted. fig_param = m.plot_parameters() Multiplicative Future Regressors Future regressors can also be added in multiplicative mode. You simply need to set the mode to multiplicative when adding the regressors to the NeuralProphet object. m = m.add_future_regressor(name='A', mode=\"multiplicative\") m = m.add_future_regressor(name='B') In the above example, we have both additive and multiplicative regressors, where A is multiplicative and B is additive. All the other steps in the fitting and the forecasting processes are the same. Regularization for Future Regressors We can add regularization into the future regressors as below. m = m.add_future_regressor(name='A', regularization=0.05) m = m.add_future_regressor(name='B', regularization=0.02) This will add sparsity into the individual regressor coefficients.","title":"Future Regressors"},{"location":"model/future-regressors/#modelling-future-regressors","text":"Future regressors are the external variables which have known future values. In that sense, the future regressors functionality if very similar to special events. The past values of these regressors corresponding to the training time stamps, have to be provided along with the training data itself. See below for an example where we create two dummy regressors A and B by taking rolling means of the original data. df['A'] = df['y'].rolling(7, min_periods=1).mean() df['B'] = df['y'].rolling(30, min_periods=1).mean() The dataframe created likewise, should look like below. ds y A B 0 2007-12-10 9.59076 9.59076 9.59076 1 2007-12-11 8.51959 9.05518 9.05518 2 2007-12-12 8.18368 8.76468 8.76468 3 2007-12-13 8.07247 8.59162 8.59162 4 2007-12-14 7.89357 8.45201 8.45201 In order to perform forecasting, we also need to provide the future values of the regressors. future_regressors_df = pd.DataFrame(data={'A': df['A'][:50], 'B': df['B'][:50]}) This dataframe looks like below. A B 0 9.59076 9.59076 1 9.05518 9.05518 2 8.76468 8.76468 3 8.59162 8.59162 4 8.45201 8.45201 It is a dataframe with only the columns of the future values of the regressors. Similar to events, future regressors too can be added in both the additive and multiplicative formats.","title":"Modelling Future Regressors"},{"location":"model/future-regressors/#additive-future-regressors","text":"The default mode for future regressors in neural_prophet is additive. The regressors have to be added to the NeuralProphet object by calling the add_future_regressor function. Once this is done, the model can be fitted by providing to the fit function, the dataframe of the training data as well as the regressor values. m = NeuralProphet( n_forecasts=10, yearly_seasonality=False, weekly_seasonality=False, daily_seasonality=False, ) m = m.add_future_regressor(name='A') m = m.add_future_regressor(name='B') metrics = m.fit(df, freq=\"D\") When forecasting, the future dataframe must be created by providing the future values of the regressors. To do that, now you need to call the make_future_dataframe function by providing the previously created future_regressors_df as an argument. future = m.make_future_dataframe(df=df, regressors_df=future_regressors_df, periods=3) forecast = m.predict(df=future) Now you can plot the components the same way as before and the resulting plot would look something like below. fig_comp = m.plot_components(forecast) In addition to the trend it also shows a plot for the additive future regressors. The coefficients of the future regressors can also be plotted. fig_param = m.plot_parameters()","title":"Additive Future Regressors"},{"location":"model/future-regressors/#multiplicative-future-regressors","text":"Future regressors can also be added in multiplicative mode. You simply need to set the mode to multiplicative when adding the regressors to the NeuralProphet object. m = m.add_future_regressor(name='A', mode=\"multiplicative\") m = m.add_future_regressor(name='B') In the above example, we have both additive and multiplicative regressors, where A is multiplicative and B is additive. All the other steps in the fitting and the forecasting processes are the same.","title":"Multiplicative Future Regressors"},{"location":"model/future-regressors/#regularization-for-future-regressors","text":"We can add regularization into the future regressors as below. m = m.add_future_regressor(name='A', regularization=0.05) m = m.add_future_regressor(name='B', regularization=0.02) This will add sparsity into the individual regressor coefficients.","title":"Regularization for Future Regressors"},{"location":"model/lagged-regressors/","text":"Modelling Lagged Regressors In the current state of NeuralProphet development, Lagged Regressor support is only available when the AR-Net is enabled. This is because they are both handled in a similar way internally using Feed-Forward Neural Networks and need to specify the n_lags value. For simplicity, at the moment we use the same n_lags value for both the AR-Net and the Lagged Regressors. Therefore, with Lagged Regressors, the NeuralProphet object is instantiated similar with AR-Net like below. m = NeuralProphet( n_forecasts=3, n_lags=5, yearly_seasonality=False, weekly_seasonality=False, daily_seasonality=False, ) When fitting the model, the dataframe provided to the fit function should have additional columns for your lagged regressors like below. ds y A 0 2007-12-10 00:00:00 9.59076 9.59076 1 2007-12-11 00:00:00 8.51959 9.05518 2 2007-12-12 00:00:00 8.18368 8.76468 3 2007-12-13 00:00:00 8.07247 8.59162 4 2007-12-14 00:00:00 7.89357 8.45201 In this example, we have a Lagged Regressor named A . You also need to register these Lagged Regressors with the NeuralProphet object by calling the add_lagged_regressor function and giving the necessary configs. m = m.add_lagged_regressor(name='A') By setting the only_last_value argument of the add_lagged_regressor function, the user can specify either to use only the last known value of the regressor within the input window or else use the same number of lags as auto-regression. Now you can perform the model fitting and forecasting as usual. The plotted components should look like below. You can see the components corresponding to both auto-regression and the Lagged Regressor A . The coefficients plot looks like below. It shows both the AR and Lagged Regressor relevance at the 5 lags corresponding to the input window.","title":"Lagged Regressors"},{"location":"model/lagged-regressors/#modelling-lagged-regressors","text":"In the current state of NeuralProphet development, Lagged Regressor support is only available when the AR-Net is enabled. This is because they are both handled in a similar way internally using Feed-Forward Neural Networks and need to specify the n_lags value. For simplicity, at the moment we use the same n_lags value for both the AR-Net and the Lagged Regressors. Therefore, with Lagged Regressors, the NeuralProphet object is instantiated similar with AR-Net like below. m = NeuralProphet( n_forecasts=3, n_lags=5, yearly_seasonality=False, weekly_seasonality=False, daily_seasonality=False, ) When fitting the model, the dataframe provided to the fit function should have additional columns for your lagged regressors like below. ds y A 0 2007-12-10 00:00:00 9.59076 9.59076 1 2007-12-11 00:00:00 8.51959 9.05518 2 2007-12-12 00:00:00 8.18368 8.76468 3 2007-12-13 00:00:00 8.07247 8.59162 4 2007-12-14 00:00:00 7.89357 8.45201 In this example, we have a Lagged Regressor named A . You also need to register these Lagged Regressors with the NeuralProphet object by calling the add_lagged_regressor function and giving the necessary configs. m = m.add_lagged_regressor(name='A') By setting the only_last_value argument of the add_lagged_regressor function, the user can specify either to use only the last known value of the regressor within the input window or else use the same number of lags as auto-regression. Now you can perform the model fitting and forecasting as usual. The plotted components should look like below. You can see the components corresponding to both auto-regression and the Lagged Regressor A . The coefficients plot looks like below. It shows both the AR and Lagged Regressor relevance at the 5 lags corresponding to the input window.","title":"Modelling Lagged Regressors"},{"location":"model/seasonality/","text":"Modelling Seasonality Seasonality in NeuralProphet is modelled using Fourier terms. It can be specified both in additive and multiplicative modes. Additive Seasonality The default mode for seasonality is additive. See below for a minimalistic example of additive seasonality in NeuralProphet. m = NeuralProphet() metrics = m.fit(df, freq=\"D\") You can see both the weekly and yearly seasonal shapes. Since required seasonality is not explicitly stated in the model development, NeuralProphet fits any seasonality that is possible with the data. The model also assigns default values to the number of Fourier terms desired for every seasonality. You can also specify these numbers as in the below example. m = NeuralProphet( yearly_seasonality=8, weekly_seasonality=3 ) According to this example, yearly seasonal pattern will use 8 Fourier terms and the weekly seasonal pattern will use 3 Fourier terms. By playing around with the number of Fourier terms, you can either underfit or overfit the seasonality. Below is an example where the seasonality is overfitted for the same data, with a high number of Fourier terms for each seasonality. m = NeuralProphet( yearly_seasonality=16, weekly_seasonality=8 ) Multiplicative Seasonality Seasonality can also be modelled multiplicatively by setting the mode explicitly like below. By doing this, the seasonality will be multiplicative with respect to the trend. m = NeuralProphet( seasonality_mode='multiplicative' ) Regularize Seasonality Just like all the other components in NeuralProphet, seasonality too can be regularized. This is done by regularizing the Fourier coefficients like below. For the details on how to set the seasonality_reg parameter, refer to the Section on Hyperparameter Selection . m = NeuralProphet( yearly_seasonality=16, weekly_seasonality=8, daily_seasonality=False, seasonality_reg=1, )","title":"Seasonality"},{"location":"model/seasonality/#modelling-seasonality","text":"Seasonality in NeuralProphet is modelled using Fourier terms. It can be specified both in additive and multiplicative modes.","title":"Modelling Seasonality"},{"location":"model/seasonality/#additive-seasonality","text":"The default mode for seasonality is additive. See below for a minimalistic example of additive seasonality in NeuralProphet. m = NeuralProphet() metrics = m.fit(df, freq=\"D\") You can see both the weekly and yearly seasonal shapes. Since required seasonality is not explicitly stated in the model development, NeuralProphet fits any seasonality that is possible with the data. The model also assigns default values to the number of Fourier terms desired for every seasonality. You can also specify these numbers as in the below example. m = NeuralProphet( yearly_seasonality=8, weekly_seasonality=3 ) According to this example, yearly seasonal pattern will use 8 Fourier terms and the weekly seasonal pattern will use 3 Fourier terms. By playing around with the number of Fourier terms, you can either underfit or overfit the seasonality. Below is an example where the seasonality is overfitted for the same data, with a high number of Fourier terms for each seasonality. m = NeuralProphet( yearly_seasonality=16, weekly_seasonality=8 )","title":"Additive Seasonality"},{"location":"model/seasonality/#multiplicative-seasonality","text":"Seasonality can also be modelled multiplicatively by setting the mode explicitly like below. By doing this, the seasonality will be multiplicative with respect to the trend. m = NeuralProphet( seasonality_mode='multiplicative' )","title":"Multiplicative Seasonality"},{"location":"model/seasonality/#regularize-seasonality","text":"Just like all the other components in NeuralProphet, seasonality too can be regularized. This is done by regularizing the Fourier coefficients like below. For the details on how to set the seasonality_reg parameter, refer to the Section on Hyperparameter Selection . m = NeuralProphet( yearly_seasonality=16, weekly_seasonality=8, daily_seasonality=False, seasonality_reg=1, )","title":"Regularize Seasonality"},{"location":"model/trend/","text":"Modelling Trend This is a minimalistic example of trend modelling in Neuralprophet by defining changepoints. m = NeuralProphet( n_changepoints=100, trend_smoothness=2, yearly_seasonality=False, weekly_seasonality=False, daily_seasonality=False, ) metrics = m.fit(df, freq=\"D\") future = m.make_future_dataframe(df, periods=365, n_historic_predictions=len(df)) forecast = m.predict(future) The components plot looks like below with only trend and residuals as a components. The coefficients plot should show the coefficients corresponding to the 100 changepoints.","title":"Trend"},{"location":"model/trend/#modelling-trend","text":"This is a minimalistic example of trend modelling in Neuralprophet by defining changepoints. m = NeuralProphet( n_changepoints=100, trend_smoothness=2, yearly_seasonality=False, weekly_seasonality=False, daily_seasonality=False, ) metrics = m.fit(df, freq=\"D\") future = m.make_future_dataframe(df, periods=365, n_historic_predictions=len(df)) forecast = m.predict(future) The components plot looks like below with only trend and residuals as a components. The coefficients plot should show the coefficients corresponding to the 100 changepoints.","title":"Modelling Trend"}]}